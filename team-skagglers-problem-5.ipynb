{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multi Class Text classification\n\n### Problem Statement\nThe problem is supervised text classification problem. Given a new complaint comes in, we want to assign it to one of 12 categories. The classifier makes the assumption that each new complaint is assigned to one and only one category. This is multi-class text classification problem.\n","metadata":{}},{"cell_type":"markdown","source":"# Table of Content\n\n* [Importing packages and loading data](#packages)\n* [Exploratory Data Analysis (EDA) and Feature Engineering](#EDA)\n* [Text Preprocessing](#Pre)\n* [Multi-Classification models](#Modelling)\n    * [Spliting the data: train and test](#Split)\n    * [Models](#Model)\n* [Comparison of model performance](#Comp)\n* [Model Evaluation](#Eval)\n    * [Precision, Recall, F1-score](#Scores)\n    * [Confusion Matrix](#CM)\n* [Predictions](#Predictions)","metadata":{}},{"cell_type":"markdown","source":"<a id='imp'></a>\n## Importing packages and loading data","metadata":{}},{"cell_type":"code","source":"\nimport os\nprint(os.listdir(\"../input\"))\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import randint\nimport seaborn as sns # used for plot interactive graph. \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom io import StringIO\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import chi2\nfrom IPython.display import display\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-25T16:00:17.378200Z","iopub.execute_input":"2022-04-25T16:00:17.378738Z","iopub.status.idle":"2022-04-25T16:00:18.798245Z","shell.execute_reply.started":"2022-04-25T16:00:17.378555Z","shell.execute_reply":"2022-04-25T16:00:18.797161Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Loading the data into a dataframe\ndata = pd.read_csv('../input/consumer-complaint-database/rows.csv')\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:00:18.800023Z","iopub.execute_input":"2022-04-25T16:00:18.800537Z","iopub.status.idle":"2022-04-25T16:00:40.868892Z","shell.execute_reply.started":"2022-04-25T16:00:18.800499Z","shell.execute_reply":"2022-04-25T16:00:40.867066Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"We have more than 1 million instances (rows) and 18 features (columns).","metadata":{}},{"cell_type":"markdown","source":"<a id='eda'></a>\n## Exploratory Data Analysis (EDA) and Feature Engineering","metadata":{}},{"cell_type":"code","source":"data.head() # Analysing our data by viewing the top 5 columns to get a rough idea on our columns and their datatypes","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:00:40.870604Z","iopub.execute_input":"2022-04-25T16:00:40.870918Z","iopub.status.idle":"2022-04-25T16:00:40.939982Z","shell.execute_reply.started":"2022-04-25T16:00:40.870881Z","shell.execute_reply":"2022-04-25T16:00:40.938021Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"The dataset given to us contains 18 features or columns out of which only some are necessary for performing our multiclass text classifcation. As we can clearly see that the columns like Date, ZIP code are very irrelevant for our classification problem. SO we are only concerned with the columns Product and Consumer Complaint.\n\n","metadata":{}},{"cell_type":"code","source":"# Creating a new dataframe with only the 2 columns as mentioned above\ndata1 = data[['Product', 'Consumer complaint narrative']].copy()","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2022-04-25T16:00:40.942416Z","iopub.execute_input":"2022-04-25T16:00:40.942720Z","iopub.status.idle":"2022-04-25T16:00:41.144210Z","shell.execute_reply.started":"2022-04-25T16:00:40.942684Z","shell.execute_reply":"2022-04-25T16:00:41.142828Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data1.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:00:41.146729Z","iopub.execute_input":"2022-04-25T16:00:41.147008Z","iopub.status.idle":"2022-04-25T16:00:41.754204Z","shell.execute_reply.started":"2022-04-25T16:00:41.146971Z","shell.execute_reply":"2022-04-25T16:00:41.751700Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"As we can see there are a lot of Null values in the Consumer Complaints Narrative column. So if we directly feed this data to our Machine Learning model it will result in improper results. So we handle the missing values first.","metadata":{}},{"cell_type":"code","source":"# Handling the Missing values by only considering the ones having non null values\ndata1 = data1[pd.notnull(data1['Consumer complaint narrative'])]\n\n# Renaming second column for a simpler name\ndata1.columns = ['Product', 'Consumer_complaint'] \n\ndata1.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:00:41.764200Z","iopub.execute_input":"2022-04-25T16:00:41.767989Z","iopub.status.idle":"2022-04-25T16:00:42.302361Z","shell.execute_reply.started":"2022-04-25T16:00:41.767937Z","shell.execute_reply":"2022-04-25T16:00:42.300940Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Finding the percentage of problems with textual data in it\ntotal = data1['Consumer_complaint'].notnull().sum()\nround((total/len(data)*100),1)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:00:42.304475Z","iopub.execute_input":"2022-04-25T16:00:42.304768Z","iopub.status.idle":"2022-04-25T16:00:42.486291Z","shell.execute_reply.started":"2022-04-25T16:00:42.304730Z","shell.execute_reply":"2022-04-25T16:00:42.485245Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Out of all the 1 million consumer complaints we had, there are about 380K cases that have textual complaints which is roughly 30% of the dataset which contains not null values.\n","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(data.Product.unique()).values","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:00:42.492062Z","iopub.execute_input":"2022-04-25T16:00:42.495272Z","iopub.status.idle":"2022-04-25T16:00:42.879815Z","shell.execute_reply.started":"2022-04-25T16:00:42.495217Z","shell.execute_reply":"2022-04-25T16:00:42.878782Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"We can clearly see that there are 18 different complaint categories that is our target. But by careful observation you can easily see that some of the categories occur in a vaster category as well. For example, there are 2 categories 'Credit Card' and 'Prepaid Card'. But these all occur in the 'Credit Card or prepaid card' category as well. So whenever a consumer complaint arise about Credit Card, it will either be classified in 'Credit Card' or 'Credit Card or prepaid card' categories and it would be absolutely correct. But this will affect our model performance. So we merge the sub categroies into a complete one category to improve our model performance.","metadata":{}},{"cell_type":"code","source":"# The computation will be very time consuming given the amount of data. So we applied the concept of rando sampling\n# where we sampled our entire data into smaller and easily computable samples.\ndata2 = data1.sample(10000, random_state=1).copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:00:42.883891Z","iopub.execute_input":"2022-04-25T16:00:42.887282Z","iopub.status.idle":"2022-04-25T16:00:42.943475Z","shell.execute_reply.started":"2022-04-25T16:00:42.887236Z","shell.execute_reply":"2022-04-25T16:00:42.942477Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Renaming the sub categories to be merged in the vaster categories\ndata2.replace({'Product': \n             {'Credit reporting, credit repair services, or other personal consumer reports': \n              'Credit reporting, repair, or other', \n              'Credit reporting': 'Credit reporting, repair, or other',\n             'Credit card': 'Credit card or prepaid card',\n             'Prepaid card': 'Credit card or prepaid card',\n             'Payday loan': 'Payday loan, title loan, or personal loan',\n             'Money transfer': 'Money transfer, virtual currency, or money service',\n             'Virtual currency': 'Money transfer, virtual currency, or money service'}}, \n            inplace= True)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:00:42.950168Z","iopub.execute_input":"2022-04-25T16:00:42.959793Z","iopub.status.idle":"2022-04-25T16:00:42.993645Z","shell.execute_reply.started":"2022-04-25T16:00:42.959737Z","shell.execute_reply":"2022-04-25T16:00:42.992684Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data2.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:00:42.994949Z","iopub.execute_input":"2022-04-25T16:00:42.995251Z","iopub.status.idle":"2022-04-25T16:00:43.008177Z","shell.execute_reply.started":"2022-04-25T16:00:42.995212Z","shell.execute_reply":"2022-04-25T16:00:43.007375Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(data2.Product.unique())","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:00:43.020830Z","iopub.execute_input":"2022-04-25T16:00:43.021298Z","iopub.status.idle":"2022-04-25T16:00:43.059716Z","shell.execute_reply.started":"2022-04-25T16:00:43.021254Z","shell.execute_reply":"2022-04-25T16:00:43.058679Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"The number of classes are reduced from 18 to 13. <br><br>Now we need to represent each class as a number, so as our predictive model can better understand the different categories.","metadata":{}},{"cell_type":"code","source":"# Creating a new column 'category_id' with encoded categories \ndata2['category_id'] = data2['Product'].factorize()[0]\ncategory_id_data = data2[['Product', 'category_id']].drop_duplicates()\n\n\n# Dictionaries for future use\ncategory_to_id = dict(category_id_data.values)\nid_to_category = dict(category_id_data[['category_id', 'Product']].values)\n\n# New dataframe\ndata2.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:00:43.063585Z","iopub.execute_input":"2022-04-25T16:00:43.066835Z","iopub.status.idle":"2022-04-25T16:00:43.123240Z","shell.execute_reply.started":"2022-04-25T16:00:43.066789Z","shell.execute_reply":"2022-04-25T16:00:43.122349Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,12))\nsns.countplot(x='category_id', data=data2,order= data2['category_id'].value_counts().index)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:19:41.983262Z","iopub.execute_input":"2022-04-25T16:19:41.983518Z","iopub.status.idle":"2022-04-25T16:19:42.211224Z","shell.execute_reply.started":"2022-04-25T16:19:41.983489Z","shell.execute_reply":"2022-04-25T16:19:42.210512Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"The bar chart below shows the number of complaints per category.It can be observed that most of customer complaints are due to:\n* credit reporting, credit repair\n* debt collection \n* mortgage ","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(8,6))\ncolors = ['grey','grey','grey','grey','grey','grey','grey','grey','grey',\n    'grey','darkblue','darkblue','darkblue']\ndata2.groupby('Product').Consumer_complaint.count().sort_values().plot.barh(\n    ylim=0, color=colors, title= 'NUMBER OF COMPLAINTS IN EACH PRODUCT CATEGORY\\n')\nplt.xlabel('Number of ocurrences', fontsize = 10);","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:00:43.799034Z","iopub.execute_input":"2022-04-25T16:00:43.803478Z","iopub.status.idle":"2022-04-25T16:00:44.547848Z","shell.execute_reply.started":"2022-04-25T16:00:43.803431Z","shell.execute_reply":"2022-04-25T16:00:44.546492Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:00:44.549753Z","iopub.execute_input":"2022-04-25T16:00:44.550064Z","iopub.status.idle":"2022-04-25T16:00:44.580884Z","shell.execute_reply.started":"2022-04-25T16:00:44.550025Z","shell.execute_reply":"2022-04-25T16:00:44.579696Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"figure = plt.figure(figsize=(16,16))\ntext = \" \".join(cat.split()[0] for cat in data2['Consumer_complaint'])\nword_cloud = WordCloud(collocations = False, background_color = 'white').generate(text)\nplt.imshow(word_cloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:00:44.585632Z","iopub.execute_input":"2022-04-25T16:00:44.585948Z","iopub.status.idle":"2022-04-25T16:00:47.013961Z","shell.execute_reply.started":"2022-04-25T16:00:44.585906Z","shell.execute_reply":"2022-04-25T16:00:47.013099Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"<a id='pre'></a>\n## Text Preprocessing\n\nThe text needs to be transformed to vectors so as the algorithms will be able make predictions. In this case it will be used the Term Frequency – Inverse Document Frequency (TFIDF) weight to evaluate __how important a word is to a document in a collection of documents__.\n\nAfter removing __punctuation__ and __lower casing__ the words, importance of a word is determined in terms of its frequency.","metadata":{}},{"cell_type":"markdown","source":"### “Term Frequency – Inverse Document Frequency \n\n__TF-IDF__ is the product of the __TF__ and __IDF__ scores of the term.<br><br> $$\\text{TF-IDF}=\\frac{\\text{TF}}{\\text{IDF}}$$<br>\n\n__Term Frequency :__ This summarizes how often a given word appears within a document.\n\n$$\\text{TF} = \\frac{\\text{Number of times the term appears in the doc}}{\\text{Total number of words in the doc}}$$<br><br>\n__Inverse Document Frequency:__ This downscales words that appear a lot across documents. A term has a high IDF score if it appears in a few documents. Conversely, if the term is very common among documents (i.e., “the”, “a”, “is”), the term would have a low IDF score.<br>\n\n$$\\text{IDF} = \\ln\\left(\\frac{\\text{Number of docs}}{\\text{Number docs the term appears in}} \\right)$$<br>\n\nTF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents. The higher the TFIDF score, the rarer the term is. For instance, in a Mortgage complaint the word _mortgage_ would be mentioned fairly often. However, if we look at other complaints, _mortgage_ probably would not show up in many of them. We can infer that _mortgage_ is most probably an important word in Mortgage complaints as compared to the other products. Therefore, _mortgage_ would have a high TF-IDF score for Mortgage complaints.\n\nTfidfVectorizer class can be initialized with the following parameters:\n* __min_df__: remove the words from the vocabulary which have occurred in less than ‘min_df’ number of files.\n* __max_df__: remove the words from the vocabulary which have occurred in more than _‘max_df’ * total number of files in corpus_.\n* __sublinear_tf__: set to True to scale the term frequency in logarithmic scale.\n* __stop_words__: remove the predefined stop words in 'english'.\n* __use_idf__: weight factor must use inverse document frequency.\n* __ngram_range__: (1, 2) to indicate that unigrams and bigrams will be considered.","metadata":{}},{"cell_type":"code","source":"vec = TfidfVectorizer(sublinear_tf=True, min_df=5,\n                        ngram_range=(1, 2), \n                        stop_words='english')\n\n# We transform each complaint into a vector to provide a numerical weightage to the textual data\nfeatures = vec.fit_transform(data2.Consumer_complaint).toarray()\n\nlabels = data2.category_id\n\nprint(\"Each of the %d complaints is represented by %d features (TF-IDF score of unigrams and bigrams)\" %(features.shape))","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:00:47.015197Z","iopub.execute_input":"2022-04-25T16:00:47.015737Z","iopub.status.idle":"2022-04-25T16:00:59.490546Z","shell.execute_reply.started":"2022-04-25T16:00:47.015701Z","shell.execute_reply":"2022-04-25T16:00:59.489639Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Finding the three most correlated terms with each of the product categories\nN = 3\nfor Product, category_id in sorted(category_to_id.items()):\n  features_chi2 = chi2(features, labels == category_id)\n  indices = np.argsort(features_chi2[0])\n  feature_names = np.array(vec.get_feature_names())[indices]\n  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n  print(\"\\n==> %s:\" %(Product))\n  print(\"  * Most Correlated Unigrams are: %s\" %(', '.join(unigrams[-N:])))\n  print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(bigrams[-N:])))","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:00:59.495402Z","iopub.execute_input":"2022-04-25T16:00:59.499019Z","iopub.status.idle":"2022-04-25T16:01:28.899756Z","shell.execute_reply.started":"2022-04-25T16:00:59.498947Z","shell.execute_reply":"2022-04-25T16:01:28.898714Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"<a id='ml'></a>\n## Multi-Classification models\n\nThe classification models we used are:\n* Random Forest\n* Linear Support Vector Machine\n* Multinomial Naive Bayes \n* Logistic Regression.","metadata":{}},{"cell_type":"markdown","source":"<a id='Split'></a>\n### Spliting the data into train and test sets\nThe original data was divided into features (X) and target (y), which were then splitted into train (75%) and test (25%) sets. Thus, the algorithms would be trained on one set of data and tested out on a completely different set of data.","metadata":{}},{"cell_type":"code","source":"X = data2['Consumer_complaint'] # Collection of documents\ny = data2['Product'] # Target or the labels we want to predict (i.e., the 13 different complaints of products)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25,random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:01:28.905270Z","iopub.execute_input":"2022-04-25T16:01:28.907812Z","iopub.status.idle":"2022-04-25T16:01:28.923956Z","shell.execute_reply.started":"2022-04-25T16:01:28.907762Z","shell.execute_reply":"2022-04-25T16:01:28.922418Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"<a id='Model'></a>\n### Modelling","metadata":{}},{"cell_type":"code","source":"models = [\n    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0),\n    LinearSVC(),\n    MultinomialNB(),\n    LogisticRegression(random_state=0),\n]\n\n# 5 Cross-validation\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\n\nentries = []\nfor model in models:\n  model_name = model.__class__.__name__\n  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n  for fold_idx, accuracy in enumerate(accuracies):\n    entries.append((model_name, fold_idx, accuracy))\n    \ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:01:28.929941Z","iopub.execute_input":"2022-04-25T16:01:28.932207Z","iopub.status.idle":"2022-04-25T16:17:55.139757Z","shell.execute_reply.started":"2022-04-25T16:01:28.932163Z","shell.execute_reply":"2022-04-25T16:17:55.138811Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"<a id='comp'></a>\n## Comparison of model performance\n\nThe best mean accuracy was obtained with LinearSVC.","metadata":{}},{"cell_type":"code","source":"mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\nstd_accuracy = cv_df.groupby('model_name').accuracy.std()\n\nacc = pd.concat([mean_accuracy, std_accuracy], axis= 1, ignore_index=True)\nacc.columns = ['Mean Accuracy', 'Standard deviation']\nacc","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:17:55.140928Z","iopub.execute_input":"2022-04-25T16:17:55.141180Z","iopub.status.idle":"2022-04-25T16:17:55.162432Z","shell.execute_reply.started":"2022-04-25T16:17:55.141139Z","shell.execute_reply":"2022-04-25T16:17:55.161050Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.boxplot(x='model_name', y='accuracy', \n            data=cv_df, \n            color='lightblue', \n            showmeans=True)\nplt.title(\"MEAN ACCURACY (cv = 5)\\n\", size=14);","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:17:55.163338Z","iopub.execute_input":"2022-04-25T16:17:55.163734Z","iopub.status.idle":"2022-04-25T16:17:55.436119Z","shell.execute_reply.started":"2022-04-25T16:17:55.163700Z","shell.execute_reply":"2022-04-25T16:17:55.435302Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"<a id='Eval'></a>\n## Model Evaluation","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(features, \n                                                               labels, \n                                                               data2.index, test_size=0.25, \n                                                               random_state=1)\nmodel = LinearSVC()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:17:55.437373Z","iopub.execute_input":"2022-04-25T16:17:55.437609Z","iopub.status.idle":"2022-04-25T16:17:57.426556Z","shell.execute_reply.started":"2022-04-25T16:17:55.437576Z","shell.execute_reply":"2022-04-25T16:17:57.425646Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"<a id='m'></a>\n### Precision, Recall, F1-score","metadata":{}},{"cell_type":"code","source":"# Classification report\nprint('\\t\\t\\t\\tCLASSIFICATIION METRICS\\n')\nprint(metrics.classification_report(y_test, y_pred,target_names= data2['Product'].unique()))","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:17:57.427978Z","iopub.execute_input":"2022-04-25T16:17:57.428419Z","iopub.status.idle":"2022-04-25T16:17:57.451478Z","shell.execute_reply.started":"2022-04-25T16:17:57.428383Z","shell.execute_reply":"2022-04-25T16:17:57.450777Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"It is possible to observe that the classes with more support (number of occurrences) tend to have a better f1-cscore. This is because the algorithm was trained with more data.<br><br>\nThe classes that can be classified with more precision are __‘Mortgage’__, __‘Credit reporting, repair, or other’__, and __‘Student loan’__.","metadata":{}},{"cell_type":"markdown","source":"<a id='cm'></a>\n### Confusion Matrix\n\nA Confusion Matrix is a table which rows represent the actual class and columns represents the predicted class.<br><br>\nIf we had a perfect model that always classifies correctly a new complaint, then the confusion matrix would have values in the diagonal only (where predicted label = actual label).","metadata":{}},{"cell_type":"code","source":"conf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(8,8))\nsns.heatmap(conf_mat, annot=True, cmap=\"Accent\", fmt='d',\n            xticklabels=category_id_data.Product.values, \n            yticklabels=category_id_data.Product.values)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title(\"CONFUSION MATRIX - LinearSVC\\n\", size=16);","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:17:57.455648Z","iopub.execute_input":"2022-04-25T16:17:57.456090Z","iopub.status.idle":"2022-04-25T16:17:58.450719Z","shell.execute_reply.started":"2022-04-25T16:17:57.456051Z","shell.execute_reply":"2022-04-25T16:17:58.450060Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"In general, the confusion matrix looks good (clear diagonal that represents correct classifications). Nevertheless, there are cases were the complaint was classified in a wrong class.\n\n#### Misclassified complaints\nWe are checking the complaints that are misclassified","metadata":{}},{"cell_type":"code","source":"for predicted in category_id_data.category_id:\n  for actual in category_id_data.category_id:\n    if predicted != actual and conf_mat[actual, predicted] >= 20:\n      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], \n                                                           id_to_category[predicted], \n                                                           conf_mat[actual, predicted]))\n    \n      display(data2.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['Product', \n                                                                'Consumer_complaint']])\n      print('')","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:17:58.454393Z","iopub.execute_input":"2022-04-25T16:17:58.455095Z","iopub.status.idle":"2022-04-25T16:17:58.515642Z","shell.execute_reply.started":"2022-04-25T16:17:58.455054Z","shell.execute_reply":"2022-04-25T16:17:58.514859Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"#### Most correlated terms with each category","metadata":{}},{"cell_type":"code","source":"model.fit(features, labels)\n\nN = 4\nfor Product, category_id in sorted(category_to_id.items()):\n  indices = np.argsort(model.coef_[category_id])\n  feature_names = np.array(vec.get_feature_names())[indices]\n  unigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 1][:N]\n  bigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 2][:N]\n  print(\"\\n==> '{}':\".format(Product))\n  print(\"  * Top unigrams: %s\" %(', '.join(unigrams)))\n  print(\"  * Top bigrams: %s\" %(', '.join(bigrams)))","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:17:58.517012Z","iopub.execute_input":"2022-04-25T16:17:58.517262Z","iopub.status.idle":"2022-04-25T16:18:01.241102Z","shell.execute_reply.started":"2022-04-25T16:17:58.517227Z","shell.execute_reply":"2022-04-25T16:18:01.240297Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"<a id='Predictions'></a>\n## Predictions\n\nNow let's make a few predictions on unseen data.<br>","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25,\n                                                    random_state = 0)\n\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n                        ngram_range=(1, 2), \n                        stop_words='english')\n\nfitted_vectorizer = tfidf.fit(X_train)\ntfidf_vectorizer_vectors = fitted_vectorizer.transform(X_train)\n\nmodel = LinearSVC().fit(tfidf_vectorizer_vectors, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:18:01.242219Z","iopub.execute_input":"2022-04-25T16:18:01.242466Z","iopub.status.idle":"2022-04-25T16:18:05.750387Z","shell.execute_reply.started":"2022-04-25T16:18:01.242431Z","shell.execute_reply":"2022-04-25T16:18:05.749655Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"new_complaint = \"\"\"I have been enrolled back at XXXX XXXX University in the XX/XX/XXXX. Recently, i have been harassed by \\\nNavient for the last month. I have faxed in paperwork providing them with everything they needed. And yet I am still getting \\\nphone calls for payments. Furthermore, Navient is now reporting to the credit bureaus that I am late. At this point, \\\nNavient needs to get their act together to avoid me taking further action. I have been enrolled the entire time and my \\\ndeferment should be valid with my planned graduation date being the XX/XX/XXXX.\"\"\"\nprint(model.predict(fitted_vectorizer.transform([new_complaint])))","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:18:05.751484Z","iopub.execute_input":"2022-04-25T16:18:05.751871Z","iopub.status.idle":"2022-04-25T16:18:05.767159Z","shell.execute_reply.started":"2022-04-25T16:18:05.751837Z","shell.execute_reply":"2022-04-25T16:18:05.766282Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"The algorithm has classified this text as a \"Student loan\" complaint. Now let's look at the real label of this complaint.","metadata":{}},{"cell_type":"code","source":"data2[data2['Consumer_complaint'] == new_complaint]","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:18:05.770042Z","iopub.execute_input":"2022-04-25T16:18:05.771049Z","iopub.status.idle":"2022-04-25T16:18:05.795297Z","shell.execute_reply.started":"2022-04-25T16:18:05.771014Z","shell.execute_reply":"2022-04-25T16:18:05.794507Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"Our model was correct, the complaint was about **Student Loan**. Note that this customer has used terms with high TFIDF score, such us **Navient[](http://)**.<br><br>\n","metadata":{}},{"cell_type":"code","source":"new_complaint_2 = \"\"\"Equifax exposed my personal information without my consent, as part of their recent data breach. \\\nIn addition, they dragged their feet in the announcement of the report, and even allowed their upper management to sell \\\noff stock before the announcement.\"\"\"\nprint(model.predict(fitted_vectorizer.transform([new_complaint_2])))","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:18:05.796552Z","iopub.execute_input":"2022-04-25T16:18:05.796923Z","iopub.status.idle":"2022-04-25T16:18:05.804782Z","shell.execute_reply.started":"2022-04-25T16:18:05.796884Z","shell.execute_reply":"2022-04-25T16:18:05.803892Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"data2[data2['Consumer_complaint'] == new_complaint_2]","metadata":{"execution":{"iopub.status.busy":"2022-04-25T16:18:05.806210Z","iopub.execute_input":"2022-04-25T16:18:05.807055Z","iopub.status.idle":"2022-04-25T16:18:05.821526Z","shell.execute_reply.started":"2022-04-25T16:18:05.807018Z","shell.execute_reply":"2022-04-25T16:18:05.820803Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"Again, the algorithm correctly classified the caomplaint as __\"Credit reporting, repair, or other\"__. Note that this customer has used terms with high TFIDF score, such us __equifax, report__. <br><br>\nAlthough our model is not going to be all the time correct when classifying new complaints, it does a good job.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}